# softmax  
初始softmax算子，理论上每个线程计算自己对应的val = expf(x[idx])，然后调用atomicAdd把值加到在共享内存的sum上。
atomicAdd 是原子加法，保证即使多个线程同时访问，也不会出现写冲突或丢失更新。逻辑上是并行累加所有的值。

但是原子操作在同一个内存地址上会串行化，所有线程都在同一个共享内存地址 sum 上进行原子加法，且重复计算了expf(x[idx])
```
template <const int NUM_THREADS = 256>
__global__ void naive_softmax_kernel(const float* x, float* y, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;

  __shared__ float sum;
  if (threadIdx.x == 0) {
    sum = 0.0f;
  }
  __syncthreads();


  if (idx < N) {
    float val = expf(x[idx]);
    atomicAdd(&sum, val);
  }
  __syncthreads();


  if (idx < N) {
    y[idx] = expf(x[idx]) / sum;
  }
}
```
```
----------------------------------------------------------------------------------------------------
                                             S=4096, H=256
----------------------------------------------------------------------------------------------------
       out_softmax_naive: ['0.00963387  ', '0.00886965  ', '0.00062277  '], time:0.52869081ms
       out_softmax_torch: ['0.00963387  ', '0.00886965  ', '0.00062277  '], time:0.04095078ms
```
利用共享内存存储expf(x[idx])，发现算子提升不高。
```
template <const int NUM_THREADS = 256>
__global__ void naive_softmax_kernel(const float* x, float* y, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;

  __shared__ float sum;
  __shared__ float exp_vals[NUM_THREADS];
  if (threadIdx.x == 0) {
    sum = 0.0f;
  }
  __syncthreads();


  if (idx < N) {
    float val = expf(x[idx]);
    exp_vals[threadIdx.x] = val;
    atomicAdd(&sum, val);
  }
  __syncthreads();


  if (idx < N) {
    y[idx] = expf(exp_vals[threadIdx.x] )/ sum;
  }
}
```
```
----------------------------------------------------------------------------------------------------
                                             S=4096, H=256
----------------------------------------------------------------------------------------------------
       out_softmax_naive: ['0.00623726  ', '0.00286294  ', '0.00224289  '], time:0.55024624ms
       out_softmax_naive_1: ['0.00623726  ', '0.00286294  ', '0.00224289  '], time:0.51517248ms
       out_softmax_torch: ['0.00623726  ', '0.00286294  ', '0.00224289  '], time:0.05508900ms
```

算力绝对足够，怀疑是block线程数太大，很多线程并不在计算。
我尝试了下把block_size增大和减小。

```
block_size = 256
----------------------------------------------------------------------------------------------------
                                             S=4096, H=256
----------------------------------------------------------------------------------------------------
       out_softmax_naive: ['0.00143825  ', '0.00481496  ', '0.00187181  '], time:0.52022457ms
     out_softmax_naive_1: ['0.00143825  ', '0.00481496  ', '0.00187181  '], time:0.52367687ms
out_softmax_block_reduce: ['0.00143825  ', '0.00481496  ', '0.00187181  '], time:0.52000046ms
       out_softmax_torch: ['0.00143825  ', '0.00481495  ', '0.00187181  '], time:0.04999161ms

block_size = 64
----------------------------------------------------------------------------------------------------
                                             S=4096, H=256
----------------------------------------------------------------------------------------------------
       out_softmax_naive: ['0.01186977  ', '0.01217986  ', '0.00658958  '], time:0.30803919ms
     out_softmax_naive_1: ['0.01186977  ', '0.01217987  ', '0.00658958  '], time:0.27533770ms
out_softmax_block_reduce: ['0.01186977  ', '0.01217987  ', '0.00658958  '], time:0.29001474ms
       out_softmax_torch: ['0.00364985  ', '0.0037452   ', '0.00202624  '], time:0.04999638ms

block_size = 32
----------------------------------------------------------------------------------------------------
                                             S=4096, H=256
----------------------------------------------------------------------------------------------------
       out_softmax_naive: ['0.00590369  ', '0.00864032  ', '0.01646961  '], time:0.27999401ms
     out_softmax_naive_1: ['0.00590369  ', '0.00864032  ', '0.01646961  '], time:0.28512716ms
out_softmax_block_reduce: ['0.00590369  ', '0.00864032  ', '0.01646961  '], time:0.27999401ms
       out_softmax_torch: ['0.00162526  ', '0.00237864  ', '0.004534    '], time:0.05000114ms

```
